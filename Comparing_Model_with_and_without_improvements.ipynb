{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU3UaZnNx3OF",
        "outputId": "1bdb2dd6-79e6-40af-ea66-8e4fe34f0240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import faiss\n",
        "import time\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "LIPSCHITZ_CONSTANT = 1.0"
      ],
      "metadata": {
        "id": "WOlc9-INybUM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRACEAdaptor(nn.Module):\n",
        "    def __init__(self, hidden_dim, epsilon_init=0.5, lipschitz_constant=LIPSCHITZ_CONSTANT):\n",
        "        super(GRACEAdaptor, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.epsilon_init = epsilon_init\n",
        "        self.lipschitz_constant = lipschitz_constant\n",
        "        self.codebook = []\n",
        "        self.values = nn.ParameterList([])\n",
        "        self.epsilon = []\n",
        "        self.index = faiss.IndexFlatL2(hidden_dim)\n",
        "\n",
        "    def forward(self, h_l_minus_1):\n",
        "        if not self.codebook:\n",
        "            return h_l_minus_1\n",
        "\n",
        "\n",
        "        hidden_state_np = h_l_minus_1.cpu().detach().numpy()\n",
        "\n",
        "\n",
        "        D, I = self.index.search(hidden_state_np, 1)\n",
        "\n",
        "\n",
        "        min_distance = D[0][0]\n",
        "        min_idx = I[0][0]\n",
        "\n",
        "\n",
        "        if min_distance < self.epsilon[min_idx]:\n",
        "\n",
        "            value_update = self.values[min_idx] * self.lipschitz_constant\n",
        "            h_l = h_l_minus_1 + value_update\n",
        "            return h_l\n",
        "        else:\n",
        "            return h_l_minus_1\n",
        "\n",
        "    def update_codebook(self, h_l_minus_1, new_value):\n",
        "\n",
        "        hidden_state_np = h_l_minus_1.cpu().detach().numpy().astype('float32')\n",
        "\n",
        "        if not self.codebook:\n",
        "            self.codebook.append(hidden_state_np)\n",
        "            self.values.append(nn.Parameter(new_value))\n",
        "            self.epsilon.append(self.epsilon_init)\n",
        "            self.index.add(hidden_state_np)\n",
        "        else:\n",
        "            D, I = self.index.search(hidden_state_np, 1)\n",
        "            min_distance = D[0][0]\n",
        "            min_idx = I[0][0]\n",
        "\n",
        "            if min_distance < self.epsilon[min_idx]:\n",
        "                self.epsilon[min_idx] += self.epsilon_init\n",
        "                self.values[min_idx] = nn.Parameter(new_value)\n",
        "            else:\n",
        "                self.codebook.append(hidden_state_np)\n",
        "                self.values.append(nn.Parameter(new_value))\n",
        "                self.epsilon.append(self.epsilon_init)\n",
        "                self.index.add(hidden_state_np)\n"
      ],
      "metadata": {
        "id": "ZF06cxN_ykmB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRACEModelWrapper(nn.Module):\n",
        "    def __init__(self, model, grace_layers=None, lipschitz_constant=LIPSCHITZ_CONSTANT):\n",
        "        super(GRACEModelWrapper, self).__init__()\n",
        "        self.model = model\n",
        "        self.grace_layers = grace_layers or []\n",
        "        self.grace_adaptors = nn.ModuleList([GRACEAdaptor(self.model.config.hidden_size, lipschitz_constant=lipschitz_constant) for _ in self.grace_layers])\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "        hidden_states = list(outputs.hidden_states)\n",
        "\n",
        "\n",
        "        for i, grace_adaptor in enumerate(self.grace_adaptors):\n",
        "            hidden_states[self.grace_layers[i]] = grace_adaptor(hidden_states[self.grace_layers[i]])\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def update_grace(self, input_ids, true_labels, criterion, optimizer, scale_factor=100.0):\n",
        "\n",
        "        outputs = self.forward(input_ids)\n",
        "        logits = outputs.logits\n",
        "        loss = criterion(logits, true_labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def measure_performance(model, input_ids, criterion, optimizer, true_labels):\n",
        "    model.train()\n",
        "    start_time = time.time()\n",
        "\n",
        "    outputs = model(input_ids)\n",
        "    loss = criterion(outputs.logits, true_labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    predicted_class = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "    return loss.item(), predicted_class.item(), elapsed_time\n"
      ],
      "metadata": {
        "id": "D-4y5ha0ypgt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-tiny')\n",
        "model = BertForSequenceClassification.from_pretrained('prajjwal1/bert-tiny')\n",
        "\n",
        "\n",
        "grace_model_baseline = GRACEModelWrapper(model, grace_layers=[0, 1])\n",
        "\n",
        "\n",
        "grace_model_improved = GRACEModelWrapper(model, grace_layers=[0, 1])\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_baseline = optim.Adam(grace_model_baseline.parameters(), lr=1e-5)\n",
        "optimizer_improved = optim.Adam(grace_model_improved.parameters(), lr=1e-5)\n",
        "\n",
        "input_text = \"This is an example sentence.\"\n",
        "inputs = tokenizer(input_text, return_tensors='pt')\n",
        "input_ids = inputs['input_ids']\n",
        "true_labels = torch.tensor([1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NzACR4nzRqt",
        "outputId": "435916b9-02b3-45a1-809a-86ba7599c846"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_loss, baseline_prediction, baseline_time = measure_performance(grace_model_baseline, input_ids, criterion, optimizer_baseline, true_labels)\n",
        "print(f\"Baseline - Loss: {baseline_loss}, Prediction: {baseline_prediction}, Time: {baseline_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkElTaL7zWn6",
        "outputId": "d2c8fa6f-6008-4cd3-87a4-c5b6c3ea3deb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline - Loss: 0.7011982202529907, Prediction: 0, Time: 0.1268 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "improved_loss, improved_prediction, improved_time = measure_performance(grace_model_improved, input_ids, criterion, optimizer_improved, true_labels)\n",
        "print(f\"Improved - Loss: {improved_loss}, Prediction: {improved_prediction}, Time: {improved_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0iqGTAczaIH",
        "outputId": "b0d0292f-74e8-45df-d487-6810a0df71c8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improved - Loss: 0.5796358585357666, Prediction: 1, Time: 0.0922 seconds\n"
          ]
        }
      ]
    }
  ]
}