{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n"
      ],
      "metadata": {
        "id": "yVzY37AHGzPN"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRACEAdaptor(nn.Module):\n",
        "    def __init__(self, hidden_dim, epsilon_init=0.5):\n",
        "        super(GRACEAdaptor, self).__init__()\n",
        "        self.codebook = []\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.epsilon_init = epsilon_init\n",
        "        self.epsilon = []\n",
        "        self.values = nn.ParameterList([])\n",
        "\n",
        "    def forward(self, h_l_minus_1):\n",
        "        if not self.codebook:\n",
        "            return h_l_minus_1\n",
        "        distances = [torch.dist(h_l_minus_1, k) for k in self.codebook]\n",
        "        min_distance = min(distances)\n",
        "        min_idx = distances.index(min_distance)\n",
        "        if min_distance < self.epsilon[min_idx]:\n",
        "            h_l = h_l_minus_1 + self.values[min_idx]\n",
        "            return h_l\n",
        "        else:\n",
        "            return h_l_minus_1\n",
        "\n",
        "    def update_codebook(self, h_l_minus_1, new_value):\n",
        "        if not self.codebook:\n",
        "            self.codebook.append(h_l_minus_1.clone().detach())\n",
        "            self.values.append(nn.Parameter(new_value))\n",
        "            self.epsilon.append(self.epsilon_init)\n",
        "        else:\n",
        "            distances = [torch.dist(h_l_minus_1, k) for k in self.codebook]\n",
        "            min_distance = min(distances)\n",
        "            min_idx = distances.index(min_distance)\n",
        "            if min_distance < self.epsilon[min_idx]:\n",
        "                self.epsilon[min_idx] += self.epsilon_init\n",
        "                self.values[min_idx] = nn.Parameter(new_value)\n",
        "            else:\n",
        "                self.codebook.append(h_l_minus_1.clone().detach())\n",
        "                self.values.append(nn.Parameter(new_value))\n",
        "                self.epsilon.append(self.epsilon_init)\n"
      ],
      "metadata": {
        "id": "RZGPXd2iG__K"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRACEModelWrapper(nn.Module):\n",
        "    def __init__(self, model, grace_layers=None):\n",
        "        super(GRACEModelWrapper, self).__init__()\n",
        "        self.model = model\n",
        "        self.grace_layers = grace_layers or []\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "        return outputs\n",
        "\n",
        "    def update_grace(self, input_ids, true_labels, criterion, optimizer, scale_factor=100.0):\n",
        "        outputs = self.forward(input_ids)\n",
        "        logits = outputs.logits\n",
        "        loss = criterion(logits, true_labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        logits = outputs.logits.clone().detach()\n",
        "        true_class_logits = logits[:, true_labels.item()]\n",
        "        modified_logits = logits + (scale_factor * (1 - true_class_logits)).unsqueeze(1)\n",
        "        outputs.logits = modified_logits\n",
        "        print(f\"Modified logits: {modified_logits}\")\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "id": "rRZaYRxpHD-I"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-tiny')\n",
        "model = BertForSequenceClassification.from_pretrained('prajjwal1/bert-tiny')\n",
        "\n",
        "grace_model = GRACEModelWrapper(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J__mJoDsHFpa",
        "outputId": "9a4f074f-42df-41e1-a154-8299363d00d9"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(grace_model.parameters(), lr=1e-5)\n"
      ],
      "metadata": {
        "id": "yDcgg8pLHJGb"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"This is an example sentence.\"\n",
        "inputs = tokenizer(input_text, return_tensors='pt')\n",
        "input_ids = inputs['input_ids']\n"
      ],
      "metadata": {
        "id": "9skj1IDsHLpH"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = grace_model(input_ids)\n",
        "predicted_class = torch.argmax(outputs.logits, dim=-1)\n",
        "print(f\"Predicted class before correction: {predicted_class.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV7cCer4HOss",
        "outputId": "4582334c-e48d-4399-ae58-6d6114c4970e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class before correction: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels = torch.tensor([0])\n",
        "\n",
        "grace_model.update_grace(input_ids, true_labels, criterion, optimizer, scale_factor=500.0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CKk-0igHT2q",
        "outputId": "98b0cb6f-a16c-4bfa-ce41-fe69bde86dfc"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modified logits: tensor([[698.4131, 698.6824]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs_after_correction = grace_model(input_ids)\n",
        "predicted_class_after_correction = torch.argmax(outputs_after_correction.logits, dim=-1)\n",
        "print(f\"Predicted class after correction: {predicted_class_after_correction.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnevB1zVHWnX",
        "outputId": "73b35a30-cb46-4a2e-9a23-fcf03817c8d3"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class after correction: 1\n"
          ]
        }
      ]
    }
  ]
}